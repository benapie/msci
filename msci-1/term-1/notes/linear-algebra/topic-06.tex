\chapter{Real vector spaces}

\section{Vectors spaces and subspaces}

\begin{definition}
    A \textbf{real vector space} is a non-empty set $V$ together with two functions:
    \begin{enumerate}
        \item $V \times V \mapsto V$, vector addition: $(\bm{u}, \bm{v}) \mapsto \bm{u} + \bm{v}$; and
        \item $\mathbb{R} \times V \mapsto V$ scalar multiplication: $(\lambda, \bm{v}) \mapsto \lambda \bm{v}$.
    \end{enumerate}
    This set must satisfy the axioms of a real vector space (obviously) which was mentioned in Proposition \ref{pro:axioms_of_real_vector_space}.
\end{definition}

\begin{example}
    The following are examples of real vector spaces.
    \begin{enumerate}
        \item $\mathbb{R}$;
        \item vector subspaces of $\mathbb{R}$;
        \item $M_{m \times n}$ with matrix addition and scalar multiplication;
        \item $\mathbb{R}[x]_n = \text{polynomials in $x$ of degree at most $n$ with real coefficients}$;
        \item the set of all functions $f : \mathbb{R} \mapsto \mathbb{R}$ is a vector space under addition of functions and scalar multiplication; and
        \item the set of all continuous functions $f : [0,1] \mapsto \mathbb{R}$.
    \end{enumerate}
\end{example}

\begin{remark}
    A vector space can be over any set, such as the complex numbers $\mathbb{C}$ or the rationals $\mathbb{Q}$. In general, one considers vector spaces over a \textbf{field} $F$, where $F$ takes the role of $\mathbb{R}$ in real vector spaces. 
\end{remark}

\begin{definition}[Vector subspace]
    A vector subspace of a vector space $V$ is an on-empty subset which is a vector space with the same addition and scalar multiplication as $V$..
\end{definition}

\begin{definition}[Subspace criterion]
    A non-empty subset $U$ of a vector space $V$ is a vector subspace of $V$ if it satisfies the following three conditions:
    \begin{enumerate}
        \item closure under addition: if $\bm{u}_1, \bm{u}_2 \in U$ then $(\bm{u}_1 + \bm{u}_2) \in U$;
        \item closure under scalar multiplication: if $\bm{u} \in U$ and $\lambda \in \mathbb{R}$ then $\lambda \bm{u} \in U$; and
        \item existence of an origin: $\bm{0} \in U$.
    \end{enumerate}
\end{definition}

\begin{remark}
    The third condition in the subspace criterion is just a specific case of the second condition where $\lambda = 0$.
\end{remark}

\begin{example}
    The set of all $A \in M_{n}(\mathbb{R})$ that are symmetric (that is, $A = A^{\rm{T}}$) forms a vector subspace of $M_n(\mathbb{R})$.
\end{example}

\begin{example}
    The set of all even polynomials forms a vector subspace of $\mathbb[x]_n$.
\end{example}

\section{Linear independence, spanning sets, bases, and dimensions}

The notion of linear combination, linear independence, spans, and spanning sets also work in general real vector spaces.

\begin{definition}
    If $V$ is a vector space then \[\{\bm{u}_1, \bm{u}_2, \ldots, \bm{u}_k\} \subset V\] is called a \textbf{basis} for $V$ if, and only if, \[\langle \bm{u}_1, \bm{u}_2, \ldots, \bm{u}_k \rangle = V\] and \[\{\bm{u}_1, \bm{u}_2, \ldots, \bm{u}_k\}\] is linearly independent.
\end{definition}

\begin{example}
    A basis for $\mathbb{R}[x]_2$ is $\{1, x, x^2\}$ as \[ax^2 + bx + c = 0 \iff a = b = c = 0\] for all $x \in \mathbb{R}$ and any quadratic can be written as a linear combination of $\{1, x, x^2\}$ so they span $\mathbb{R}[x]_2$.
\end{example}

\begin{example}
    Are the polynomials \[p_1(x) = 1 - x, \quad p_2(x) = x + x^2, \quad p_3(x) = 1 + x^2\] linearly independent? 
    
    A linear combination of these polynomials is
    \begin{align*}
        p(x) &= \lambda_1 p_1(x) + \lambda_2 p_2(x) + \lambda_3 p_3(x) \\
        &= \lambda_1 (1 - x) + \lambda_2 (x + x^2) + \lambda_3 (1 + x^2) \\
        &= (\lambda_1 + \lambda_3) + (-\lambda_1 + \lambda_2) x + (\lambda_2 + \lambda_3)x^2.
    \end{align*}
    If $p(x) = 0$ then
    \begin{align*}
        \lambda_1 + \lambda_3 &= 0; \\
        -\lambda_1 + \lambda_2 &= 0;\\
        \lambda_3 + \lambda_3 &= 0.
    \end{align*}
    We can easily see that $\lambda_1 = \lambda_2 = -\lambda_3$ and hence $p_1(x) + p_2(x) = p_3(x)$ and the polynomials are linearly dependent.
\end{example}

\begin{example}
    Consider the following matrices in $M_{3,2}(\mathbb{R})$ 
    \[
        A_1 =
        \begin{pmatrix}
            1 & 0 \\
            0 & 0 \\
            0 & 1 \\
        \end{pmatrix}
        , \quad A_2 =
        \begin{pmatrix}
            0 & 1 \\
            1 & 0 \\
            0 & 0 \\
        \end{pmatrix}
        , \quad A_3 =
        \begin{pmatrix}
            0 & 1 \\
            0 & 0 \\
            0 & 1 \\
        \end{pmatrix}
        .
    \]
    Are these matrices linearly independent?
    
    Consider 
    \begin{align*}
        \begin{pmatrix}
            0 & 0 \\
            0 & 0 \\
            0 & 0 \\
        \end{pmatrix}
        &= \lambda_1
        \begin{pmatrix}
            1 & 0 \\
            0 & 0 \\
            0 & 1 \\
        \end{pmatrix}
        + \lambda_2
        \begin{pmatrix}
            0 & 1 \\
            1 & 0 \\
            0 & 0 \\
        \end{pmatrix}
        + \lambda_3
        \begin{pmatrix}
            0 & 1 \\
            0 & 0 \\
            0 & 1 \\
        \end{pmatrix}
        \\
        &=
        \begin{pmatrix}
            \lambda_1 & \lambda_2 + \lambda_3 \\
            \lambda_2 & 0 \\
            0 & \lambda_1 + \lambda_3, \\
        \end{pmatrix}
    \end{align*}
    it is clear to see the only solution is $\lambda_1 = \lambda_2 = \lambda_3$, therefore, these matrices are linearly independent.
\end{example}

\begin{example}
    Consider the following matrices in $M_2(\mathbb{R})$.
    \[
        A_1 = 
        \begin{pmatrix}
            1 & 0 \\
            0 & 0 \\
        \end{pmatrix}
        , \quad A_2 = 
        \begin{pmatrix}
            0 & 1 \\
            1 & 0 \\
        \end{pmatrix}
        , \quad A_3 = 
        \begin{pmatrix}
            0 & 1 \\
            -1 & 0 \\
        \end{pmatrix}
        , \quad A_4 = 
        \begin{pmatrix}
            0 & 0 \\
            0 & 1 \\
        \end{pmatrix}
        .
    \]
    Do these matrices form a basis for $M_2(\mathbb{R})$?
    
    First we must show that these matrices span $M_2(\mathbb{R})$. Given $A \in M_2(\mathbb{R})$, we need to find $\lambda_i \in \mathbb{R}$ so that
    \begin{align*}
        \begin{pmatrix}
            a & b \\
            c & d \\
        \end{pmatrix}
        &= A \\
        &= \lambda_1 A_1 + \lambda_2 A_2 + \lambda_3 A_3 + \lambda_4 A_4 \\
        &= \lambda_1
        \begin{pmatrix}
            1 & 0 \\
            0 & 0 \\
        \end{pmatrix}
        + \lambda_2 
        \begin{pmatrix}
            0 & 1 \\
            1 & 0 \\
        \end{pmatrix}
        + \lambda_3
        \begin{pmatrix}
            0 & 1 \\
            -1 & 0 \\
        \end{pmatrix}
        + \lambda_4
        \begin{pmatrix}
            0 & 0 \\
            0 & 1 \\
        \end{pmatrix}
        \\ 
        &=
        \begin{pmatrix}
            \lambda_1 & \lambda_2 + \lambda_3 \\
            \lambda_2 - \lambda_3 & \lambda_4
        \end{pmatrix}
        .
    \end{align*}
    Thus $\lambda_1 = a$, $\lambda_2 = \frac{1}{2} (b + c)$, $\lambda_3 = \frac{1}{2} (b - c)$, and $\lambda_4 = d$; therefore, these matrices span $M_2(\mathbb{R})$. Now we must show that these matrices are linearly independent. Consider
    \begin{align*}
        \begin{pmatrix}
            0 & 0 \\
            0 & 0 \\
        \end{pmatrix}
        &= \lambda_1
        \begin{pmatrix}
            1 & 0 \\
            0 & 0 \\
        \end{pmatrix}
        + \lambda_2 
        \begin{pmatrix}
            0 & 1 \\
            1 & 0 \\
        \end{pmatrix}
        + \lambda_3
        \begin{pmatrix}
            0 & 1 \\
            -1 & 0 \\
        \end{pmatrix}
        + \lambda_4
        \begin{pmatrix}
            0 & 0 \\
            0 & 1 \\
        \end{pmatrix}
        \\
        &=
        \begin{pmatrix}
            \lambda_1 & \lambda_2 + \lambda_3 \\
            \lambda_2 - \lambda_3 & \lambda_4
        \end{pmatrix}
        ,
    \end{align*}
    equating the entries we see that \[0 = \lambda_1 = \lambda_2 + \lambda_3 = \lambda_2 - \lambda_3 = \lambda_4.\] It is clear that the only solution is $\lambda_i = 0$, therefore, these matrices are linearly independent. Hence, as the matrices span $M_2(\mathbb{R})$ and are linearly independent, they form a basis for $M_2(\mathbb{R})$.
\end{example}

\begin{remark}
    Not all vector spaces have a basis, such as the set of all continuous functions $f : \mathbb{R} \mapsto \mathbb{R}$.
\end{remark}

\begin{lemma}
    Let $\bm{u}_1, \bm{u}_2, \ldots, \bm{u}_k$ be vectors in a vector space $V$, then \[\langle \bm{u}_1, \bm{u}_2, \ldots, \bm{u}_k \rangle\] is a vector subspace of $V$.
\end{lemma}

\begin{lemma}
    Let $\bm{u}_1, \bm{u}_2, \ldots, \bm{u}_k$ be vectors in a vector space $V$ and assume that one of the vectors, say $\bm{u}_1$, can be expressed as a linear combination of the others. Then \[\langle \bm{u}_2, \ldots, \bm{u}_k \rangle = \langle \bm{u}_1, \bm{u}_2, \ldots, \bm{u}_k \rangle.\]
\end{lemma}

\begin{lemma}
    Suppose that $\{\bm{u}_1, \bm{u}_2, \ldots, \bm{u}_k\}$ are not linearly independent. Then for some $1 \leq i \leq k$, we have that \[\bm{u}_i \in \langle \bm{u}_1, \bm{u}_2, \ldots, \bm{u}_{i - 1}, \bm{u}_{i + 1}, \ldots, \bm{u}_k \rangle.\]
\end{lemma}

\begin{proof}
    Since $\{\bm{u}_1, \bm{u}_2, \ldots, \bm{u}_k\}$ are not linearly independent, therefore, there exists some linear dependence relation \[\sum_{j = 1}^n \lambda_j \bm{u}_j = 0\] with at least one non-zero $\lambda_j$. Say $\lambda_i \neq 0$, then \[\bm{u}_i = \sum_{j = 1}^{i - 1} \left( \frac{\lambda_j}{-\lambda_i} \bm{u}_j \right) + \sum_{i+1}^n \left( \frac{\lambda_j}{-\lambda_i} \bm{u}_j \right).\]
\end{proof}

\begin{theorem}
    Every vector space which is generated by finitely many vectors (in others words is spanned by a finite spanning set) has a (finite) basis.
\end{theorem}

\begin{proof}
    Let $V = \langle \bm{u}_1, \bm{u}_2, \ldots, \bm{u}_k \rangle$ for some vectors $\bm{u}_i \in V$. If they are not linearly independent, then we (by an above Lemma) that there is a vector in the spanning set which lies in the span of the other elements, therefore, we can remove this vector while remaining a spanning set. We can repeat this process until our spanning set is also linearly independent.
\end{proof}

\begin{proposition}
    Let $\bm{u}_1, \bm{u}_2, \ldots, \bm{u}_k$ be linearly independent vectors in a vector space $V$ and let $\bm{u} \in V$ such that such that \[\bm{u} \not \in \langle \bm{u}_1, \bm{u}_2, \ldots, \bm{u}_k \rangle\], then \[\bm{u}_1, \bm{u}_2, \ldots, \bm{u}_k, \bm{u}\] are linearly independent.
\end{proposition}

\begin{theorem}
    Let $V$ be a vector space and $S=\{\bm{u}_1, \bm{u}_2, \ldots, \bm{u}_k\} \subset V$, then the following are equivalent:
    \begin{enumerate}
        \item $S$ is a basis for $V$;
        \item $S$ is a maximal linearly independent subset;
        \item every $\bm{u} \in V$ can be uniquely expressed as \[\bm{u} = \lambda_1 \bm{u}_1 + \lambda_2 \bm{u}_2 + \ldots + \lambda_k \bm{u}_k; \; \text{and} \]
        \item $S$ is a minimal spanning set for $V$.
    \end{enumerate}
\end{theorem}

\begin{proof}
    For $(i) \implies (ii)$ we first assume $(i)$; therefore, $S$ is linearly independent. For each $\bm{u}_{k + 1} \in V$ there exists $\lambda_i \in \mathbb{R}$ such that \[\lambda_1 \bm{u}_1 + \lambda_2 \bm{u}_2 + \ldots + \lambda_k \bm{u}_k = \bm{u}_{k + 1} \iff \lambda_1 \bm{u}_1 + \lambda_2 \bm{u}_2 + \ldots + \lambda_k \bm{u}_k - \bm{u}_{k+1} = \bm{0}.\] So $\{\bm{u}_1, \bm{u}_2, \ldots, \bm{u}_k, \bm{u}_{k+1}\}$ is not linearly independent. 
    % notes for rest
\end{proof}

\begin{definition}
    We call a vector space \textbf{finite-dimensional} if it has a finite basis.
\end{definition}

\begin{definition}
    If $\{\bm{u}_1, \bm{u}_2, \ldots, \bm{u}_k\}$ is a basis for $V$ we define the \textbf{dimension} of $V$ to be $\dim(V)=n$.
\end{definition}

\begin{theorem}[Steinitz exchange theorem]
    Suppose \[S=\{\bm{v}_1, \bm{v}_2, \ldots, \bm{v}_l\} \subset V\] is a spanning set of the vector space $V$, that is $\langle S \rangle = V$. Further suppose $\{\bm{u}_1, \bm{u}_2, \ldots, \bm{u}_k\} \subset V$ is linearly independent. Then there exists distinct vectors $\bm{v}_{i_{1}}, \bm{v}_{i_{2}}, \ldots, \bm{v}_{i_{k}} \in S$ such that for all $j$ with $0 \leq j \leq k$ we have \[(S \setminus \{\bm{v}_{i_{1}}, \bm{v}_{i_{2}}, \ldots, \bm{v}_{i_{j}}\}) \cup \{\bm{u}_1, \bm{u}_2, \ldots, \bm{u}_j\}\] is a spanning set for $V$.
\end{theorem}

The following corollaries follow directly from the Steinitz exchange theorem.

\begin{corollary}
    In the above situation, $l \geq k$.
\end{corollary}

\begin{corollary}
    If $A = \{\bm{u}_1, \bm{u}_2, \ldots, \bm{u}_m\}$ and $B = \{\bm{v}_1, \bm{v}_2, \ldots, \bm{v}_n\}$ are bases for a vector space $V$, then $m = n$.
\end{corollary}

\begin{proof}
    By the Steinitz exchange theorem (in particular the above corollary), spanning sets are always at least as big as all linearly independent sets. Therefore, as $A$ and $B$ are both bases they are both spanning sets and linearly independent. This implies that $m \geq n$ and $n \geq m$, that is, $m = n$.
\end{proof}

\begin{theorem}
    Suppose $V$ is a finite dimensional vector space, say $\dim{V}=n$. If $U$ is a subspace of $V$ then $\dim{U} \leq n$ and if $\dim{U}=n$ we have $U = V$.
\end{theorem}

% todo
% \begin{proof}
%     If $U = \{0\}$, $\dim{U}=0$. If $U \neq \{0\}$, choose any $\bm{u}_1 \in U$ where $\bm{u}_1 \neq \bm{0}$, then $\{\bm{u}_1\}$ is a linearly independent subset. I
% \end{proof}

\begin{proposition}
    Let $V$ be a vector space of dimension $n$ and \[S = \{\bm{u}_1, \bm{u}_2, \ldots, \bm{u}_k\} \subset V,\] then
    \begin{enumerate}
        \item if $k > n$, then $S$ is not linearly independent;
        
        \item if $k < n$, then $S$ does not span $V$; and
        
        \item if $k = n$, then the following is equivalent
        \begin{enumerate}
            \item $S$ is linearly independent;
            \item $S$ spans $V$; and
            \item $S$ is a basis.
        \end{enumerate}
    \end{enumerate}
\end{proposition}

\begin{proof}
    \begin{enumerate}
        \item As $V$ is $n$-dimensional, it has an $n$-element spanning set; therefore, linearly independent subsets of $V$ must have at most $n$ elements.
        
        \item As $V$ is $n$ dimensional, it has an $n$-element linearly independent set; therefore, spanning sets of must have at at least $n$ elements.
        
        \item (iii) $\implies$ (i) and (ii) $\implies$ (i) by the definition of a basis. Assuming (i), any linearly independent set has less than or equal to $n$ elements, so $S$ is a maximal linearly independent; hence, $S$ is a basis. Therefore, (i) $\iff$ (ii). Assuming (ii), if $S$ is not a basis then they are not a minimal spanning set. So there is a spanning set of $k - 1$ elements. However, $V$ has dimensional-$k$ so it contains a linearly independent set of $k$ elements and linearly independent sets cannot have a larger cardinality than spanning sets; hence, a contradiction. Therefore, (iii) $\iff$ (ii). Therefore all three statements are equivalent.
    \end{enumerate}
\end{proof}

\begin{example}
    Let $\operatorname{Sym}_2(\mathbb{R})$ be the set of all $2 \times 2$ symmetric real matrices. Consider the set of matrices 
    \[
        S =
        \left\{
            \begin{pmatrix}
                1 & 0 \\
                0 & 0 \\
            \end{pmatrix}
            , 
            \begin{pmatrix}
                0 & 1 \\
                1 & 0 \\
            \end{pmatrix}
            , 
            \begin{pmatrix}
                0 & 0 \\
                0 & 1 \\
            \end{pmatrix}
        \right\}
        \subset \operatorname{Sym}_2(\mathbb{R}).
    \]
    It is clear that $S$ is linearly independent. We can define $\operatorname{Sym}_2(\mathbb{R})$ as \[\operatorname{Sym}_2(\mathbb{R}) = \{A \in M_2(\mathbb{R}) : A^{\rm{T}} = A\};\] or, for all $A \in \operatorname{Sym}_2(\mathbb{R})$ 
    \[
        A =
        \begin{pmatrix}
            a & b \\
            c & d \\
        \end{pmatrix}
        =
        \begin{pmatrix}
            a & c \\
            b & d \\
        \end{pmatrix}
        ,
    \]
    so $b=c$; therefore,
    \[
        A =
        \begin{pmatrix}
            a & b \\
            c & d \\
        \end{pmatrix}
        = a
        \begin{pmatrix}
            1 & 0 \\
            0 & 0 \\
        \end{pmatrix}
        + b
        \begin{pmatrix}
            0 & 1 \\
            1 & 0 \\
        \end{pmatrix}
        + d
        \begin{pmatrix}
            0 & 0 \\
            0 & 1 \\
        \end{pmatrix}
        .
    \]
    This shows that $\operatorname{Sym}_2(\mathbb{R})$ has a basis of $S$ and has dimension $3$.
\end{example}

\begin{example}
    Consider \[V = \{p(x) \in \mathbb{R}[x]_n : p(1) = 0\} \subset \mathbb{R}[x]_n.\] $V$ is a vector subspace as, given $p, q \in V$ and $\lambda \in \mathbb{R}$,
    \begin{enumerate}
        \item $(p + q)(1) = p(1) + q(1) = 0$, therefore we have closure under addition; and
        \item $\lambda p(1) = \lambda 0 = 0$, so we have closure under scalar multiplication.
    \end{enumerate}
    Using calculus, it clear to see that \[V = \{(x - 1) q(x) : q(x) \in \mathbb{R}[x]_{n - 1}\}\] and $\{(x - 1), (x - 1) x, (x - 1) x^2, \ldots, (x - 1) x^{n - 1}\} \subset V$ is a basis and $\dim{V}=\dim{\mathbb{R}[x]_{n-1}}=n$.
\end{example}

\section{Bases and dimensions}

\begin{proposition}
    Suppose \[V = \{\bm{v}_1, \bm{v}_2, \ldots, \bm{v}_n\} \in \mathbb{R}^n.\] $V$ is a basis for $\mathbb{R}^n$ if, and only if, $\det{A} \neq 0$ where
    $
        A =
        \begin{pmatrix}
            \bm{v}_1 & \bm{v}_2 & \ldots & \bm{v}_n
        \end{pmatrix}
    $.
\end{proposition}

\begin{proof}
    $V$ is a basis for $\mathbb{R}^n$ $\iff$ $V$ is linearly independent $\iff$ the only solution to $A \bm{\lambda} = \bm{0}$ is $\bm{\lambda} = \bm{0}$.
\end{proof}

\begin{example}
    Is 
    \[
        \left\{
            \begin{pmatrix}
                3 \\ 1 \\ 0 \\
            \end{pmatrix}
            ,
            \begin{pmatrix}
                2 \\ 1 \\ 1 \\
            \end{pmatrix}
            ,
            \begin{pmatrix}
                0 \\ 1 \\ 0 \\
            \end{pmatrix}
        \right\}
    \]
    a basis for $\mathbb{R}^3$?
    
    \[
        \begin{vmatrix}
            3 & 2 & 0 \\
            1 & 1 & 1 \\
            0 & 1 & 0 \\
        \end{vmatrix}
        = (-1)
        \begin{vmatrix}
            3 & 0 \\
            1 & 1 \\
        \end{vmatrix}
        = -3 \neq 0;
    \]
    therefore, it is a basis for $\mathbb{R}^3$.
\end{example}

\begin{definition}
    Let $A \in M_{m \times k}(\mathbb{R})$.
    \begin{enumerate}
        \item The \textbf{column space} of $A$ is the subspace of $\mathbb{R}^n$ spanned by the columns of $A$: \begin{align*}
            \operatorname{colspace}{A} &= \{\bm{b} \in \mathbb{R}^n : A \bm{x} = \bm{b} \; \text{has a solution}\} \\
            &= \{A \bm{x} : \bm{x} \in \mathbb{R}^k\}.
        \end{align*}
        We call the dimension of this space the \textbf{column rank}, \[\operatorname{colrank} = \dim{(\operatorname{colspace}{A})}.\]
        
        \item The \textbf{row space} of $A$ is the subspace of $\mathbb{R}^n$ spanned by the rows of $A$ (more correctly, the transpose of the rows): \[\operatorname{rowspace}{A} = \operatorname{colspace}{A^{\rm{T}}}.\]
        We call the dimension of this space the \textbf{row rank}\[\operatorname{rowrank} = \dim{(\operatorname{rowspace}{A})}.\]
        
        \item The \textbf{nullspace} or \textbf{kernal} of $A$ is the solution space to the equation $A \bm{x} = \bm{0}$: \[\operatorname{nullspace}{A}=\{\bm{x} \in \mathbb{R}^n : A \bm{x} = \bm{0}\} \subset \mathbb{R}^k.\] We define the \textbf{nullity} of $A$ to be the dimension of the nullspace, \[\operatorname{nullity}(A) = \dim{(\operatorname{nullspace}{A})}.\]
    \end{enumerate}
\end{definition}

\begin{proposition}\label{pro:ero_no_change_rank}
    The EROs do not change row rank, column rank, or nullity. That is, if $E$ is an elementary matrix then
    \begin{align*}
        \operatorname{colrank}{A} &= \operatorname{colrank}{(EA)}; \\
        \operatorname{rowrank}{A} &= \operatorname{rowrank}{(EA)};\;\text{and} \\
        \operatorname{nullity}{A} &= \operatorname{nullity}{(EA)}.
    \end{align*}
\end{proposition}

\begin{corollary}
    If $P \in M_n(\mathbb{R})$ is invertible then $PA$ has the same column rank, row rank, and nullity as $A$
\end{corollary}

\begin{proof}
    Any invertible matrix is a product of EROs.
\end{proof}

% todso
% \begin{proof}[Proof of Proposition \ref{pro:ero_no_change_rank}]
%     To prove that row rank remains unchanged, note that each row $EA$ is a linear combination of the rows of $A$, so
%     \begin{align*}
%         \operatorname{rowspace}{(EA)} &\subset \operatorname{rowspace}{(A)} \\
%         \operatorname{rowspace}{(A)} & = \operatorname{rowspace}{(E^{-1}(EA))} \\
%         & \subset \operatorname{rowspace}{(EA)};
%     \end{align*}
%     therefore, \[\operatorname{rowspace}{A} = \operatorname{rowspace}{(EA)}.\]
    
%     To prove that column rank remains unchanged, consider \[\operatorname{colspace}{A} = \{A \bm{x} : \bm{x} \in \mathbb{R}^k\} \subset \mathbb{R}^n.\] Let $\{\bm{b}_1, \bm{b}_2, \ldots, \bm{b}_r\}$ be a basis of $\operatorname{colspace}{A}$, so $\operatorname{colrank} = r$. 
% \end{proof}

\begin{theorem}
    The column rank and row rank are equal, \[\operatorname{colrank}{A} = \operatorname{rowrank}{A}\] for all $A \in M_{m \times n}(\mathbb{R})$.
\end{theorem}

\begin{definition}
    We define the rank of A to be \[\rank{A} = \operatorname{colrank}{A} = \operatorname{rowrank}{A}.\]
\end{definition}

\begin{theorem}[Rank-nullity theorem]
    Given $A \in M_{n \times k}(\mathbb{R})$ then \[\rank{A} + \nullity{A} = k.\]
\end{theorem}

\begin{proof}
    Elementary row operations do not change row rank, column rank, or nullity. So we can assume $A$ is in row-reduced echelon form.
\end{proof}
