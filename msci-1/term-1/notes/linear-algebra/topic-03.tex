\chapter{Matrices and matrix algebra}

\section{Matrices}

\begin{definition}
    A matrix is a rectangular array of numbers, symbols, or expressions. The numbers, symbols, or expressions inside of a matrix are referred to as \textbf{elements} or \textbf{entries}. If a matrix has $m$ rows and $n$ columns, we call it a $m\times n$ matrix.
\end{definition}

\begin{example}
    Following are examples of matrices.
    
    \begin{enumerate}
        \item 
        \[
            A=
            \begin{pmatrix}
                1\\2\\4\\7
            \end{pmatrix}
        \]
        
        \item
        \[
            B=
            \begin{pmatrix}
                0&1&3&8&-\pi
            \end{pmatrix}
        \]
        
        \item
        \[
            C=
            \begin{pmatrix}
                1&\pi&\sqrt{2}\\
                5&e&\sqrt{111}\\
            \end{pmatrix}
        \]
    \end{enumerate}
\end{example}

\begin{definition}
    If a matrix has 1 column, we call it a \textbf{column vector}. Similarly, if a matrix has 1 row, we call it a \textbf{row vector}.
\end{definition}

\begin{definition}
    We define the the \textbf{transpose} of a $m\times n$ matrix $A$ to be a $n\times m$ matrix $A^{\rm T}$ whose $(i,j)$th entry is the $(j,i)$th entry of $A$. If $A=A^{\rm T}$ we call $A$ \textbf{symmetric}.
\end{definition}

\begin{remark}
    Note that any symmetric $m\times n$ matrix must be a square matrix, in other words, $m=n$.
\end{remark}

\begin{example}
    The transpose of 
    \[
        A=
        \begin{pmatrix}
            1&5&11\\
            \pi&e&-7\\
            \sqrt 2&\sqrt{111}&e^\pi\\
            -1&0&0\\
        \end{pmatrix}
    \]
    is
    \[
        A^{\rm T}=
        \begin{pmatrix}
            1&\pi&\sqrt 2&-1\\
            5&e&\sqrt{111}&0\\
            11&-7&e^\pi&0\\
        \end{pmatrix}
    \]
    .
\end{example}

\begin{example}
    \[
        B=
        \begin{pmatrix}
            1&e&2&10\\
            e&5&3&6\\
            2&3&\pi&1\\
            10&6&1&0\\
        \end{pmatrix}
        =B^{\rm T},
    \]
    therefore, $B$ is symmetric.
\end{example}

\begin{definition}
    Given the function $f(i,j)$, $(f(i,j))_{m\times n}$ describes a $m\times n$ matrix whose $(r,s)$th entry is $f(r,s)$.
\end{definition}

\begin{example}
    Following are some matrix descriptions and their full form.
    \begin{enumerate}
        \item
        \[
            (i+j)_{3\times2}=
            \begin{pmatrix}
                2&3&4\\3&4&5\\
            \end{pmatrix}
        \]
        
        \item 
        \[
            (i^2j)_{2\times4}=
            \begin{pmatrix}
                1&2&3&4\\4&8&12&16\\
            \end{pmatrix}
        \]
        
        \item
        \[
            (3+j)_{2\times 2}=
            \begin{pmatrix}
                4&5\\4&5\\
            \end{pmatrix}
        \]
    \end{enumerate}
\end{example}

\begin{definition}
    Define $O_{m\times n}$ to be the $m\times n$ matrix with all entries $0$, we refer to this as the zero matrix.
\end{definition}

\begin{definition}
    Define $I_m$ to be the $m\times m$ matrix who's $(i,i)th$ entry is 1 and all other entries are 0. In other words, the $m\times m$ who's $(i,j)$th entry is
    \[
        \begin{cases}
            0&i\neq j\\
            1&i=j\\
        \end{cases}.
    \]
    We refer to this as the identity matrix.
\end{definition}

We write $M_{m\times n}(\mathbb R)$ or $M_{m,n}(\mathbb R)$ for the set of all real $m\times n$ matrices. We also write $M_n(\mathbb R)$ for the set of square matrices of size $n$, $M_n(\mathbb R)=M_{n\times n}(\mathbb R)$.

\section{Matrix addition and scalar multiplication}

\begin{definition}
    \textbf{Matrix addition} is a function \[+:\mathbb M_{m\times n}(\mathbb R)\times M_{m\times n}(\mathbb R)\mapsto M_{m\times n}(\mathbb R)\] that is defined by \[(a_{ij})_{m\times n}+(b_{ij})_{m\times n}=(a_{ij}+b_{ij})_{m\times n}.\]
\end{definition}

\begin{definition}
    \textbf{Scalar multiplication} for matrices is a function \[\cdot:\mathbb R\times M_{m\times n}(\mathbb R)\mapsto M_{m\times n}(\mathbb R)\] that is defined by \[\lambda(a_{ij})_{m\times n}=(\lambda a_{ij})_{m\times n}.\]
\end{definition}

\begin{proposition}
    Matrix addition and scalar multiplication for $M_{m\times n}(\mathbb R)$ satisfies the same properties as vector addition and scalar multiplication for $\mathbb R^n$, see Proposition \ref{pro:axioms_of_real_vector_space}.
\end{proposition}

\begin{remark}
    Note that \[(\lambda A)^{\rm T}=\lambda(A^{\rm T})\] and \[(A+B)^{\rm T}=A^{\rm T}+B^{\rm T}.\]
\end{remark}

\section{Matrix multiplication}

\begin{definition}
    \textbf{Matrix multiplication} is a function \[\cdot:M_{m\times n}(\mathbb R)\times M_{n\times p}(\mathbb R)\mapsto M_{m\times p}(\mathbb R).\] Suppose $X\in M_{m\times n}(\mathbb R),Y\in M_{n\times p}(\mathbb R)$ where
    \begin{align*}
        X&=(x_{ij})_{m\times n},&Y=(y_{ij})_{n\times p},
    \end{align*}
    then
    \begin{align*}
        XY&=(x_{ij})_{m\times n}(y_{ij})_{n\times p}\\
        &=\left(\sum_{r=1}^n{x_{ir}y_{rj}}\right)_{m \times p}
    \end{align*}
    or, in natural language, the $(i,j)$th entry of $XY$ is the $i$th row of $X$ `multiplied by' the $j$th column of $Y$.
\end{definition}

\begin{proposition}[Axioms of matrix multiplication]
Let $X,X'\in M_{m\times n}(\mathbb R)$, $Y,Y'\in M_{n\times p}(\mathbb R)$, $Z\in M_{p\times q}(\mathbb R)$, and $\lambda,\lambda'\in\mathbb R$. Then,
    \begin{enumerate}
        \item associativity: $(XY)Z=X(YZ)$;
        \item $I_mX=X=XI_n$;
        \item $0_{p\times m}X=O_{p\times n}$, $XO_{n\times p}=O_{m\times p}$;
        \item $(XY)^{\rm T}=Y^{\rm T}X^{\rm T}$;
        \item $\lambda(XY)=(\lambda X)Y=X(\lambda Y)$;
        \item $(X+X')Y=XY+X'Y$, $X(Y+Y')=XY+XY'$; and
        \item $\lambda(X+X')=\lambda X+\lambda X'$, $(\lambda+\lambda')X=\lambda X+\lambda' X$.
    \end{enumerate}
\end{proposition}

\begin{remark}
    It is important to note here that \[XY\neq YX\] for all $X\in M_{m\times n}$ $Y\in M_{n\times p}(\mathbb R)$, in fact, $YX$ is only defined for $m=p$.
\end{remark}

\begin{example}
    \begin{align*}
        \begin{pmatrix}
            1&2\\
        \end{pmatrix}
        \begin{pmatrix}
            3\\4\\
        \end{pmatrix}
        &=
        \begin{pmatrix}
            11\\
        \end{pmatrix}
        \\
        \begin{pmatrix}
            3\\4\\
        \end{pmatrix}
        \begin{pmatrix}
            1&2\\
        \end{pmatrix}
        &=
        \begin{pmatrix}
            3&6\\4&8\\
        \end{pmatrix}
    \end{align*}
\end{example}

\begin{remark}
    It is also important to note that \[XY=O_{m\times p}\] does not imply that $X=O_{m\times n}$ or $Y=O_{n\times p}$.
\end{remark}

\begin{example}
    \[
        \begin{pmatrix}
            11&0\\5&0\\
        \end{pmatrix}
        \begin{pmatrix}
            0&0\\2&1\\
        \end{pmatrix}
        =
        \begin{pmatrix}
            0&0\\0&0\\
        \end{pmatrix}
        =O_{2\times 2}
    \]
\end{example}

From the definition of matrix multiplication, a system of linear equations can be written in the form \[A\bm x=\bm b\] where
\[
    \bm x=
    \begin{pmatrix}
        x_1\\x_2\\\vdots\\x_n\\
    \end{pmatrix}
    ,\quad\bm b=
    \begin{pmatrix}
        b_1\\b_2\\\vdots\\b_n\\
    \end{pmatrix}
    ,\quad A=
    \begin{pmatrix}
        a_{11}&a_{12}&\ldots&a_{1n}\\
        a_{21}&a_{22}&\ldots&a_{2n}\\
        \vdots&\vdots&\ddots&\vdots\\
        a_{m1}&a_{m2}&\ldots&a_{mn}\\
    \end{pmatrix}
    .
\]

\section{Inverse matrix}

\begin{definition}
    Let $A$ be a $n\times n$ square matrix. We say $A$ is \textbf{invertible} or \textbf{non-singular} if there exists an $n\times n$ square matrix $B$ such that \[AB=BA=I_n\] and we call such a matrix $B$ the inverse of $A$ and we write $A^{-1}$ for this inverse.
\end{definition}

\begin{example}
    Consider
    \[
        A=
        \begin{pmatrix}
            3&-12\\1&-4\\
        \end{pmatrix}
        .
    \]
    Note that 
    \[
        A
        \begin{pmatrix}
            4\\1\\
        \end{pmatrix}
        =
        \begin{pmatrix}
            0\\0\\
        \end{pmatrix}
        .
    \]
    If $B$ were an inverse for $A$, then we would have
    \begin{align*}
        \begin{pmatrix}
            4\\1\\
        \end{pmatrix}
        &=I_2
        \begin{pmatrix}
            4\\1\\
        \end{pmatrix}
        \\
        &=(BA)
        \begin{pmatrix}
            4\\1\\
        \end{pmatrix}
        \\
        &=B\left(A
        \begin{pmatrix}
            4\\1\\
        \end{pmatrix}
        \right)
        \\
        &=B
        \begin{pmatrix}
            0\\0\\
        \end{pmatrix}
        \\
        &=
        \begin{pmatrix}
            0\\0\\
        \end{pmatrix}
        .
    \end{align*}
    We have a contradiction, hence $A$ is not invertible.
\end{example}

\begin{proposition}
    Suppose $A\in M_n(\mathbb R)$, then
    \begin{enumerate}
        \item if $\;\exists\;\bm v\in\mathbb R^n,\bm v\neq\bm 0$ with $A\bm v=\bm 0$ then $A$ is not invertible; equivalently
        \item if $A$ is invertible, the only to $A\bm x=\bm 0$ is $\bm x=\bm 0$.
    \end{enumerate}
\end{proposition}

\begin{proof}
    \begin{enumerate}
        \item Suppose $A$ has inverse $A^{-1}$. Then if $A\bm v=\bm 0$ for $\bm v\neq 0$ we have \[\bm v=I_n\bm v=(BA)\bm v=B(A\bm v)=B\bm 0=\bm 0.\] We have a contradiction, hence $A$ is not invertible.
        
        \item Let $A^{-1}$ be an inverse for $A$. Then if $A\bm x=\bm 0$ we have \[\bm x=I_n\bm x=(A^{-1}A)\bm x=A^{-1}(A\bm x)=B^{-1}\bm 0=\bm 0.\]
    \end{enumerate}
\end{proof}

\begin{proposition}
    \begin{enumerate}
        \item A matrix has at most one inverse.
        \item If $A,B\in M_n(\mathbb R)$ that are both invertible, then $AB$ is invertible and $(AB)^{-1}=B^{-1}A^{-1}$.
        \item If $A$ is invertible then so is $A^{\rm T}$ and $(A^{\rm T})^{-1}=(A^{-1})^{\rm T}$.
        \item If $A,B\in M_n(\mathbb R)$ and $BA=I_n$ then $AB=I_n$ (so $B=A^{-1}$).
    \end{enumerate}
\end{proposition}

\begin{proof}
    \begin{enumerate}
        \item Suppose $A\in M_n{\mathbb R}$ has two inverses $A_1^{-1},A_2^{-1}\in M_n(\mathbb R)$. Then \[A_1^{-1}=A_1^{-1}I_n=A_1^{-1}(AA_2^{-1})=(A_1^{-1}A)(A_2^{-1})=I_nA_2^{-1}=A_2^{-1}.\]
        
        \item If $B^{-1}A^{-1}$ is the inverse of $AB$ then \[(AB)(B^{-1}A^{-1})=A(BB^{-1})A^{-1}=AA^{-1}=I_n\] as required.
        
        \item If $(A^{-1})^{\rm T}$ is the inverse of $(A^{\rm T})$ then \[(A^{-1})^{\rm T}A^{\rm T}=(AA^{-1})^{\rm T}=(I_n)^{\rm T}=I_n\] as required.
        
        \item We will come back to this.
    \end{enumerate}
\end{proof}

\section{Inverse matrices and linear systems}

\begin{lemma}
    Consider an arbitrary $m\times n$ linear system $A\bm x=\bm b$. Then
    \begin{enumerate}
        \item If $B\in M_{m\times m}(\mathbb R)$ is invertible then the system \[(BA)\bm x=(B\bm b)\] has the same solution set as the original system.
        
        \item If $A$ is invertible ($m=n$) then $A\bm x=\bm b$ has a unique solution \[\bm x=A^{-1}\bm b.\]
    \end{enumerate}
\end{lemma}

\begin{proof}
    \begin{enumerate}
        \item If $\bm v$ is a solution to $A\bm x=\bm b$ then $A\bm v=\bm b$ hence $BA\bm v=B\bm b$ so $\bm v$ is a solution to $(BA)\bm x=(B\bm b)$.
        
        \item For this, we apply (i) with $B=A^{-1}$.
    \end{enumerate}
\end{proof}

Recall each elementary row operation took inputs of a $m\times n$ matrix and gave an $m\times n$ matrix as an output. Each ERO is some function \[f:M_{m\times n}(\mathbb R)\mapsto M_{m\times n}(\mathbb R).\] It turns out each $f$ is given by left multiplication by some invertible matrix $E$ \[f:A\mapsto EA\] called an \textbf{elementary matrix}. Following is the corresponding elementary matrices for each ERO.

\begin{enumerate}
    \item The row operation $P_{rs}$ corresponding to the ERO $P_{rs}$ is the matrix given by \[P_{rs}=(a_{ij})\] where
    \begin{enumerate}
        \item $a_{rs}=a_{sr}=1$;
        \item $a_{ii}=1\;\forall\;i\neq r,s$; and
        \item all other entries are $0$.
    \end{enumerate}
    
    \item The elementary matrix $M_r(\lambda)$ is given by \[M_r(\lambda)\] where
    \begin{enumerate}
        \item $a_{ij}=1\;\forall\;i\neq r$;
        \item $a_{rr}=\lambda$; and
        \item all other entries are $0$.
    \end{enumerate}
    
    \item The elementary matrix $A_{rs}(\lambda)$ is given by \[A_{rs}(\lambda)=(a_{ij})\] where
    \begin{enumerate}
        \item $a_{ii}=1$;
        \item $a_{sr}=\lambda$; and
        \item all other entries are $0$.
    \end{enumerate}
\end{enumerate}

\begin{remark}
    Each elementary matrix has an elementary matrix as its inverse.
\end{remark}

\section{The fundamental theorem of invertible matrices}

\begin{theorem}
    Let $A\in M_n(\mathbb R)$, then the following are equivalent.
    \begin{enumerate}
        \item In each column of the RREF of $A$, there is a leading 1.
        \item The RREF of $A$ is $I_n$.
        \item The only solution to $A\bm x=\bm 0$ is $\bm x=\bm 0$.
        \item $A$ is invertible.
    \end{enumerate}
\end{theorem}

\begin{proof}
    \begin{description}
        \item[$(i)\iff(ii)$] This is clear from the definition of RREF.
        
        \item[$(ii)\iff(iii)$] If the RREF form of $A$ is $I_n$ then solutions to
        $\begin{pmatrix}[c|c]
            A&\bm 0
        \end{pmatrix}$
        are the same as solutions to
        $\begin{pmatrix}[c|c]
            I_n&\bm 0
        \end{pmatrix}$
        , but this has unique solution $\bm x=\bm 0$.
        
        \item[$(iii)\iff(iv)$] Assume $(iii)$ holds and for a contradiction assume not all columns have a leading $1$. So we have at least $1$ free variable in our solution set. So the solution set has more than one element; this is a contradiction. Now we have $(i)\iff(ii)\iff(iii)$.
        
        \item[$(iv)\iff(iii)$] Suppose $A$ is invertible. If $A\bm x=\bm 0$ then $\bm x=(A^{-1}A)\bm x=A^{-1}(A\bm x)=A^{-1}\bm 0=\bm 0$
        
        \item[$(ii)\implies(iv)$] Assume $(ii)$, so there exists some elementary matrices $E_1,\ldots E_R$ such that \[E_RE_{R-1}\ldots E_2E_1A=I_n,\] then
        \begin{align*}
            AE_RE_{R-1}\ldots E_1&=(E_1^{-1}E_2^{-1}\ldots E_R^{-1}E_RE_{R-1}\ldots E_1)A(E_RE_{R-1}\ldots E_2E_1)\\
            &=E_1^{-1}E_2^{-1}\ldots E_R^{-1}(E_RE_{R-1}\ldots E_1A)E_RE_{R-1}\ldots E_2E_1\\
            &=E_1^{-1}E_2^{-1}\ldots E_R^{-1}E_RE_{R-1}\ldots E_2E_1=I_n.\\
        \end{align*}
    \end{description}
\end{proof}

\begin{corollary}
    Let $A\in M_n(\mathbb R)$. $A$ is invertible if and only if $A$ is a product of elementary matrices.
\end{corollary}

\begin{proof}
    \begin{align*}
        A&=(A^{-1})^{-1}\\
        &=(E_R\ldots E_2E_1)^{-1}\\
        &=E_1^{-1}E_2^{-1}\ldots E_R^{-1},\\
    \end{align*}
    therefore, $A$ is some product of elementary matrices.
\end{proof}

\begin{proposition}
    If $BA=I_n$ then $AB=I_n$ where $A,B\in M_n(\mathbb R)$.
\end{proposition}

\begin{proof}
    \[A\bm x=\bm 0\implies \bm x=BA\bm x=B\bm 0=\bm 0.\] Hence $A$ is invertible and we have \[AB=ABAA^{-1}=A(BA)A^{-1}=AA^{-1}=I_n.\]
\end{proof}

\section{Algorithm for determining the inverse of a matrix}

Let $A\in M_n(\mathbb R)$. We can use the the Gauss-Jordan elimination for matrix $A$ to decided whether an inverse exists and, if so, calculate it. Concretely, we run the elimination on the augmented matrix 
\[
    \begin{pmatrix}[c|c]
        A&I_n\\
    \end{pmatrix}
\]
to get it in the form (if $A$ is invertible) 
\[
    \begin{pmatrix}[c|c]
        I_n&E_RE_{R-1}\ldots E_1\\
    \end{pmatrix}
    .
\]

\begin{example}
    Let
    \[
        \begin{pmatrix}
            2&1&1\\
            1&2&3\\
            0&1&2\\
        \end{pmatrix}
        .
    \]
    Here we carry out Gauss-Jordan elimination to reduce this to $I_3$:
    \begin{align*}
        \begin{pmatrix}[ccc|ccc]
            2&1&1&1&0&0\\
            1&2&3&0&1&0\\
            0&1&2&0&0&1\\
        \end{pmatrix}
        &\eroarrow{P_{21}}{}
        \begin{pmatrix}[ccc|ccc]
            1&2&3&0&1&0\\
            2&1&1&1&0&0\\
            0&1&2&0&0&1\\
        \end{pmatrix}
        \\
        &\eroarrow{A_{12}(-2)}{}
        \begin{pmatrix}[ccc|ccc]
            1&2&3&0&1&0\\
            0&-3&-5&1&-2&0\\
            0&1&2&0&0&1\\
        \end{pmatrix}
        \\
        &\eroarrow{P_{32}}{}
        \begin{pmatrix}[ccc|ccc]
            1&2&3&0&1&0\\
            0&1&2&0&0&1\\
            0&-3&-5&1&-2&0\\
        \end{pmatrix}
        \\
        &\eroarrow{A_{21}(-2)}{A_{23}(3)}
        \begin{pmatrix}[ccc|ccc]
            1&0&-1&0&1&-2\\
            0&1&2&0&0&1\\
            0&0&1&1&-2&3\\
        \end{pmatrix}
        \\
        &\eroarrow{A_{31}(1)}{A_{32}(-2)}
        \begin{pmatrix}[ccc|ccc]
            1&0&0&1&-1&1\\
            0&1&0&-2&4&-5\\
            0&0&1&1&-2&3\\
        \end{pmatrix}
        ,
    \end{align*}
    therefore,
    \[
        A^{-1}=
        \begin{pmatrix}
            1&-1&1\\
            -2&4&-5\\
            1&-2&3\\
        \end{pmatrix}
    \]
\end{example}
