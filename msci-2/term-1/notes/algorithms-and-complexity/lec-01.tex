\chapter{Asymptotic notation}
\lecture{1}{14/10}

Asymptotic notation is assumed knowledge from previous modules, 
this chapter serves as a brief recap. 
Let's start with some definitions.

\section{Problems and algorithms}

\begin{definition}[Algorithm]
    An \textbf{algorithm} is a program that accepts inputs and produce outputs.
\end{definition}

We call an algorithm \textbf{correct} if it halts with the correct output.

We use algorithm to solve \textbf{problems}. 
A \textbf{problem} can be thought of as a relationship between the inputs and outputs of an algorithm.

\begin{example}[Sorting]
    Here we introduce the widely studied \textbf{sorting problem}. 
    Let our input be a $n$-tuple $(a_1, a_2, \ldots, a_n)$. 
    The output to our algorithm should be the tuple 
    $(a_{i_1}, a_{i_2}, \ldots, a_{i_n})$ 
    where $(i_1, i_2, \ldots, i_n)$ is a reordering of $(1, 2, \ldots, n)$ such that 
    \[ a_{i_1} \leq a_{i_2} \leq \ldots \leq a_{i_n}. \]

    There are many examples of sorting algorithms that we have already studied:
    \begin{enumerate}
        \item insertion sort;
        \item merge sort; and
        \item meap sort.
    \end{enumerate}
\end{example}

Now that we know what problems and algorithms are, let's reintrouce asymptotic notation.

\section{Big-$O$ and friends}

\begin{definition}[Big-$O$]
    Let $f(n), g(n) \geq 0$. 
    We define big-$O$ of $g(n)$ as 
    \[ O(g(n)) = 
        \{
        f(n) : \;\exists\; k > 0 \; \exists \; N \in \N \;\forall\; n > N : 
        \lvert f(n) \rvert \leq k g(n)
        \}.
    \] 
    We mathematically describe this by saying that 
    $\lvert f(n) \rvert$ 
    is bounded above by $g(n)$ up to a constant factor asymptotically. 
    We can also define big-$O$ using the following limits definition: 
    \[ f(n) \in O(g(n)) \iff 
        \limsup_{n\to\infty} \left(\frac{\lvert f(n) \rvert}{g(n)}\right) < \infty; 
    \] 
    however, note that big-$O$ can not be rigorously defined using limits.
\end{definition}

You will most likely see the notation $f(n) = O(g(n))$ in literature (and this course!). 
This is a convention to mean $f(n) \in O(g(n))$.

\begin{definition}[Big-$\Omega$]
    Similarly, let $f(n), g(n) \geq 0$. 
    Then 
    \[\Omega(g(n)) = \{ f(n) : \;\exists\; k > 0 \;\exists\; N \in \N \;\forall\; n > N : f(n) \geq k g(n) \}.\] 
    Here we say that $f$ is bounded below by $g$ asymptotically.
\end{definition}

As a very loose analogy, we can look at Big-$O$ as representing the relation $\leq$. 
So 
$f(n) = O(g(n))$ 
is a similar relation for functions $f, g$ to the relation $a \leq b$ for $a, b \in \R$. 
Similarly, we can see big-$\Omega$ as representing the $\geq$ relation.

\begin{definition}[Big-$\Theta$]
    Let $f(n), g(n) \geq 0$. 
    Then 
    \[ \Theta(g(n)) = \{ f(n) : f(n) \in O(g(n)) \;\text{and}\; f(n) \in \Omega(g(n)) \}. \] 
    An alternative definition is 
    \[ \Theta(g(n)) = \{ f(n) : f(n) \in O(g(n)) \;\text{and}\; g(n) \in O(f(n)). \]
\end{definition}

\begin{proposition}
    Let $f(n), g(n) \geq 0$.
    \begin{enumerate}
        \item $f(n) = \Theta(g(n)) \iff g(n) = \Theta(f(n))$; and
        \item $f(n) = O(g(n)) \iff g(n) = \Omega(f(n))$.
    \end{enumerate}
\end{proposition}

\section{Little-$o$ and friends}

Following our analogy from earlier, 
we can see little-$o$ and little-$\omega$ for functions as the $<$ and $>$ relations for $\R$.

\begin{definition}[Little-$o$]
    We define little-$o$ of a function $g(n)$ to be 
    \[ o(g(n)) = 
        \{ f(n) : 
            \;\forall\; k > 0 \;\exists\; N \in \N : 
            kg(n) \leq f(n) \;\forall\; n > N 
        \}.
    \]
\end{definition}

\begin{definition}[Little-$\omega$]
    We define little-$\omega$ of a function $g(n)$ to be 
    \[ \omega(g(n) = \{ f(n) : 
        \;\forall\; k > 0 \;\exists\; N \in \N : f(n) \leq kg(n) \;\forall\; n > N 
        \}. 
    \]
\end{definition}

\begin{proposition}
    \begin{enumerate}
        \item $f(n) = o(g(n)) \iff g(n) = \omega(f(n))$;
        \item $f(n) = O(g(n)) \iff (f(n) = \Theta(g(n)) \;\text{or}\; (f(n) = o(g(n))$; and
        \item $f(n) = \Omega(g(n)) \iff (f(n) = \Theta(g(n)) \;\text{or}\; (f(n) = \omega(g(n))$.
    \end{enumerate}
\end{proposition}

Hopefully the last two points of the above proposition helps cements the analogy between asymptotic behaviour for functions and the $>$, $<$, $\leq$, and $\geq$ relations for $\R$.

\section{Running time}

\begin{definition}[Running time]
    Let $\alpha$ be some algorithm that takes an input $I$. 
    Then we define $T(I)$ as the time taken until $\alpha$ halts. 
    We define the \textbf{worst-case running time} as 
    \[ T(n) = \max{\{T(I) : \lvert I \rvert = n\}}, \] 
    here we denote the size of $I$ as $\lvert I \rvert$.
\end{definition}

This pretty much includes the entire recap for this module, 
but there is a comment to be made on asymptotic notation. 
When we look at the asymptotic behaviour of time complexity of some algorithm we do lose some information. 
Constants can have a large effect on the running time of an algorithm for small $n$, 
and we don't necessarily always run an algorithm for large $n$. 
It is important to understand the limiting behaviour of time complexity as well though. 
