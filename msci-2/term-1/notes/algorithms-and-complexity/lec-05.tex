\chapter{Dynamic programming}
\section{Rod cuttiing}
\lecture{5}{4/11}

\begin{problem}[Informal rod cutting problem]
    The \textbf{rod cutting problem} (or \textbf{cutting stock problem}) is framed as finding the best way of cutting rods to maximise profits. 
    The lengths that can be cut have prices associated to it. 
    Given a length $n$ and a table of prices $p_i$ for $i = 1, \ldots, n$, 
    determine the maximum revenue $r_n$ obtained by cutting and selling the rods.
\end{problem}

\begin{remark}
    A hint before we really delve into this: this is just a knapsack problem.
\end{remark}

\begin{definition}[Integer partition]
    An \textbf{integer partition} of a positive integer $n$ is a list of positive integers 
    $(a_1, \ldots, a_k)$ such that $a_1 \leq a_2 \leq \ldots \leq a_k$ and 
    \[ \sum_{i = 1}^k a_i = n. \]
    We denote the number of integer partitions on $n$ as $p(n)$.
\end{definition}

\begin{example}
    \begin{enumerate}
        \item Consider $n = 1$. 
            There is only one way we can partition this, and that is $(1)$. 
            Hence $p(1) = 1$.
        \item Consider $n = 2$. 
            There is two ways we can partition this: $(2)$ and $(1, 1)$.
        \item Consider $n = 3$. 
            We have partitions $(1, 1, 1)$, $(1, 2)$, and $(3)$ so $p(3) = 3$.
        \item Consider $n = 4$. 
            We have partitions $(1, 1, 1, 1)$, $(1, 1, 2)$, $(1, 3)$, $(2, 2)$, 
            and $(4)$; hence, $p(4) = 5$.
    \end{enumerate}
\end{example}

It has been shown that 
\[ p(n) \sim \frac{1}{4n\sqrt 3} \exp{\left(\pi \sqrt{\frac{2n}3}\right)}; \]
hence, a brute force algorithm will have exponential running time. 
So this is not a good solution to the rod cutting problem; 
however, we can use \emph{dynamic programming}.

Let $r_n$ denote the maximuim revenue from cutting a length of $n$. 
Note that $r_n$ has the following properties:
\begin{enumerate}
    \item $r_0 = 0$; and
    \item $r_l = \max_{l_i \leq l}{(l_i + r_{l - l_i})}$.
\end{enumerate}

You might need to stare at (ii) for a while for it to make sense, but trust it.

\begin{definition}[Optimal substructure]
    A problem is said to have an \textbf{optimal subsctructure} if an optimal solution is constructured from optimal solutions of its subproblems.
\end{definition}

\begin{algorithm}[Naive recursive dynamic programming solution]
    \hfill
    \begin{algorithmic}[1]
        \Procedure{ROD}{$p, n$}
            \If {$n$ = 0}
                \State \Return 0
            \EndIf
            \State $q \gets -1$
            \For{$i = 1$ to $n$}
            \State $q \gets \max\{q, p[i] + \operatorname{ROD}[p, n - i]\}$
            \EndFor
            \State \Return $q$
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

Although this implements dynamic programming, it still has exponential time. 

\begin{remark}
    Dynamic programming can utilise memory to drastically reduce the running time. 
    This is called a \textbf{time-memory trade off} (or \textbf{space-memory trade off}). 
    Recall memoisation from Algorithms and Data Structures I.
\end{remark}

A dynamic programming solution runs in polynomial time if the number of distinct subproblems is polynomial and each subproblem can be solved in polynomial time.

\begin{definition}[Overlapping]
    A subproblem is called \textbf{overlapping} if two distinct problems (or more) generate it as a subproblem.
\end{definition}

\begin{definition}[Independent]
    Two subproblems are \textbf{independent} if they share no resources.
\end{definition}

\begin{example}[Merge sort]
    In the case of merge sort, all subproblems generated are independent; 
    hence, dynamic programming will not help us in anyway.
\end{example}

The first naive recursive dynamic programming solution was built top-down and is recursive. 
We can solve this problem bottom up too, 
where we calculate every length by size and store the result. 
It is clear to see the worst-case time complexity for both of these approaches 
(if we use memoisation) 
is $\Theta(n^2)$. 
We typically see that the bottom-up approach has better performance due to less overhead.
